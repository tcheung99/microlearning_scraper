{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C:\\Users\\trudi\\Downloads\\chromedriver_win32\n",
    "#driver = webdriver.Chrome(executable_path='C:/Users/trudi/Downloads/chromedriver_win32.exe')\n",
    "\n",
    "#https://www.reddit.com/r/learnpython/comments/3axumo/scraping_multiple_websites_with_beautifulsoup4/\n",
    "\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome(executable_path=r'C:/Users/trudi/chromedriver')\n",
    "# url: https://techcrunch.com/\n",
    "driver.get(\"https://techcrunch.com/\")\n",
    "content = driver.page_source \n",
    "soup = BeautifulSoup(content)\n",
    "# page = soup.find(class_=\"feature-island-main-block fi-main-block--unread\")\n",
    "# data = soup.findAll(class_=\"fi-main-block__title\")\n",
    "# print(len(page))\n",
    "# titles = [] \n",
    "\n",
    "# print(len(data))\n",
    "# for page in data:\n",
    "#     title = page.find(\"a\",{\"class\":\"post-block__title__link\"}).text\n",
    "#     titles.append(title)\n",
    "# print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "7\n",
      "9\n",
      "11\n",
      "['NASA’s first-ever all-woman spacewalk was a success', 'Who will own the future of transportation?', 'Last few hours to apply: TC Top Picks @ Disrupt Berlin 2019', 'Greylock GP Sarah Guo is as bullish on SaaS as ever', 'A set of new tools can decrypt files locked by Stop, a highly active ransomware', 'IPOs are the beginning, not the end', 'MediaLab acquires messaging app Kik, expanding its app portfolio', 'This Week in Apps: League of Legends goes mobile, Tim Cook talks to China and more', 'HuffPost is reportedly on the auction block', 'The new iPhone is ugly', 'Mercedes-Benz app glitch exposed car owners’ information to other users', 'HTC releases a cheaper blockchain phone', 'Startups Weekly: The unicorn from down under, an Uber TV show and All Raise’s expansion', 'Apple’s China stance makes for strange political alliances, as AOC and Ted Cruz slam the company', 'AI is helping scholars restore ancient Greek texts on stone tablets', 'Adam Neumann planned for his children and grandchildren to control WeWork', 'VR/AR startup valuations reach $45 billion (on paper)', 'Tilting Point acquires game monetization startup Gondola', 'Where are US fintech’s next billion-dollar startups?', 'Alphabet’s Wing begins making first commercial drone deliveries in the US', 'Daily Crunch: Zuckerberg has thoughts on free speech', 'Harley-Davidson has resumed production of the LiveWire', '$35B face data lawsuit against Facebook will proceed', 'Bloomberg Beta, now six years old, closes its third $75 million fund', 'Who will own the future of transportation?', '', '', '']\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "titles = [] \n",
    "descs = []\n",
    "\n",
    "content = driver.page_source \n",
    "soup = BeautifulSoup(content)\n",
    "#first arg is name, finds all <a> tags \n",
    "# attrs is a dict that acts like keyword args \n",
    "# for a in soup.findAll('a',href=True, attrs = {'class':'post-block__title__link'}):\n",
    "#     #for a in soup.findAll(['a','p'],href=True, attrs = )\n",
    "#     title = a.find('div', attrs = {'class': 'post-block__title__link'})\n",
    "#     #descs = a.find('div', attrs = {'class': 'post-block__content'})\n",
    "#     titles.append(title.text)\n",
    "#     #descs.append(descs.text)\n",
    "selectors = {\n",
    "    \"https://techcrunch.com/\" : {\n",
    "        1 : {\"class\" : \"feature-island-main-block fi-main-block--unread\"},\n",
    "        2 : {\"class\" : \"fi-main-block__title\"},\n",
    "        5 : {\"class\" : \"river river--homepage\"},\n",
    "        6 : {\"class\" : \"post-block post-block--image post-block--unread\"},\n",
    "        7 : {\"class\" : \"river river--homepage\"},\n",
    "        8 : {\"class\" : \"post-block post-block--featured post-block--unread\"},\n",
    "        3 : {\"class\" : \"mini-view\"},\n",
    "        4 : {\"class\" : \"mini-view__item mini-item--unread\"}\n",
    "    },\n",
    "    \"https://gizmodo.com/\" : {\n",
    "        9 : {\"class\" : \"sc-1b65c8p-5 jOgcgy\"},\n",
    "        10 : {\"class\" : \"item branded-item branded-item--gizmodo rlhps0-0 kQEtSU\"},\n",
    "        11 : {\"class\" : \"sc-1b65c8p-5 jOgcgy\"},\n",
    "        12 : {\"class\" : \"item branded-item branded-item--earther rlhps0-0 kQEtSU\"},\n",
    "        13 : {\"class\" : \"content-meta__headline__wrapper sc-1n74gvm-0 eYcrcn\"},\n",
    "        14 : {\"class\" : \"content-meta__headline medium-headline\"}\n",
    "    }\n",
    "    \n",
    "}\n",
    "\n",
    "# page = soup.find(class_=\"feature-island-main-block fi-main-block--unread\")\n",
    "# data = soup.findAll(class_=\"fi-main-block__title\")\n",
    "# print(len(data))\n",
    "# for page in data:\n",
    "#     title = page.find(\"a\",{\"class\":\"post-block__title__link\"}).text\n",
    "#     titles.append(title)\n",
    "# print(selectors)\n",
    "\n",
    "page_f = [None]\n",
    "data_f = [None]\n",
    "\n",
    "for i in range(1,13,2):\n",
    "    print(i)\n",
    "    if (i!=1 and i<9):\n",
    "        page = (soup.find(class_= selectors[\"https://techcrunch.com/\"][i][\"class\"]))\n",
    "        data = (soup.findAll(class_=selectors[\"https://techcrunch.com/\"][i+1][\"class\"]))\n",
    "        page_f.extend(page)   \n",
    "        data_f.extend(data)  \n",
    "#         print(\"yo\")\n",
    "#         print(type(page_f))\n",
    "    elif (i==1):\n",
    "        page = (soup.find(class_= selectors[\"https://techcrunch.com/\"][i][\"class\"]))\n",
    "        data = (soup.findAll(class_=selectors[\"https://techcrunch.com/\"][i+1][\"class\"]))\n",
    "        page_f = page\n",
    "        data_f = data\n",
    "#         print(\"wut\")\n",
    "#         print(page)\n",
    "#         print(data)\n",
    "#         print(type(page_f))\n",
    "    else:\n",
    "        driver.get(\"https://gizmodo.com/\")\n",
    "        content = driver.page_source \n",
    "        soup = BeautifulSoup(content)\n",
    "        page = (soup.find(class_= selectors[\"https://gizmodo.com/\"][i][\"class\"]))\n",
    "        data = (soup.findAll(class_=selectors[\"https://gizmodo.com/\"][i+1][\"class\"]))\n",
    "        page_f.extend(page)   \n",
    "        data_f.extend(data) \n",
    "\n",
    "\n",
    "for page_f in data_f:\n",
    "#     title = page_f.find(\"a\",{\"class\":\"post-block__title__link\"}).text\n",
    "    title = page_f.find(\"a\", href=True).text\n",
    "    titles.append(title)\n",
    "        \n",
    "    \n",
    "print(titles)\n",
    "print(len(titles))\n",
    "#print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "News\n",
      "Help, I Can't Unsee These Faux-Flesh Phone Cases\n",
      "Alyse Stanley\n",
      "\n",
      "Okay, Here's When the Last The Rise of Skywalker Trailer Drops\n",
      "Julie Muncy\n",
      "\n",
      "Facebook's Top Priority is Totally Its Government-Ordered Privacy Program\n",
      "Alyse Stanley\n",
      "\n",
      "Alex Garland on the Look and Feel of Devs, and Star Wars Stealing All the Good Sets\n",
      "Jill Pantozzi\n",
      "\n",
      "Hey, Reddit NSFW Communities: Imgur's Sick of Your Smut\n",
      "Alyse Stanley\n",
      "\n",
      "'Grace and Frankie' Cast Gets Their Asses Arrested for the Planet\n",
      "Yessenia Funes\n",
      "\n",
      "Uh... The Army Just Contracted With Tom DeLonge's UFO Group to Study Those 'Alien Alloys'\n",
      "Tom McKay\n",
      "['', 'News', \"Help, I Can't Unsee These Faux-Flesh Phone Cases\", 'Alyse Stanley', '', \"Okay, Here's When the Last The Rise of Skywalker Trailer Drops\", 'Julie Muncy', '', \"Facebook's Top Priority is Totally Its Government-Ordered Privacy Program\", 'Alyse Stanley', '', 'Alex Garland on the Look and Feel of Devs, and Star Wars Stealing All the Good Sets', 'Jill Pantozzi', '', \"Hey, Reddit NSFW Communities: Imgur's Sick of Your Smut\", 'Alyse Stanley', '', \"'Grace and Frankie' Cast Gets Their Asses Arrested for the Planet\", 'Yessenia Funes', '', \"Uh... The Army Just Contracted With Tom DeLonge's UFO Group to Study Those 'Alien Alloys'\", 'Tom McKay']\n"
     ]
    }
   ],
   "source": [
    "titles = [] \n",
    "descs = []\n",
    "driver = webdriver.Chrome(executable_path=r'C:/Users/trudi/chromedriver')\n",
    "\n",
    "driver.get(\"https://gizmodo.com/\")\n",
    "content = driver.page_source \n",
    "soup = BeautifulSoup(content)\n",
    "# print(soup.prettify())\n",
    "# selectors = {\"hackernews\" : \"https://news.ycombinator.com/\" }: {\n",
    "#         10 : {\"class\" : \"title\"},\n",
    "#         9 : {\"class\" : \"athing\"}\n",
    "# } \n",
    "# page = soup.find(class_=\"feature-island-main-block fi-main-block--unread\")\n",
    "# data = soup.findAll(class_=\"fi-main-block__title\")\n",
    "\n",
    "selectors = {\n",
    "    \"https://gizmodo.com/\" : {\n",
    "        1 : {\"class\" : \"sc-1b65c8p-5 jOgcgy\"},\n",
    "        2 : {\"class\" : \"item branded-item branded-item--gizmodo rlhps0-0 kQEtSU\"},\n",
    "        3 : {\"class\" : \"sc-1b65c8p-5 jOgcgy\"},\n",
    "        4 : {\"class\" : \"item branded-item branded-item--earther rlhps0-0 kQEtSU\"},\n",
    "        6 : {\"class\" : \"content-meta__headline__wrapper sc-1n74gvm-0 eYcrcn\"},\n",
    "        5 : {\"class\" : \"content-meta__headline medium-headline\"}\n",
    "#         7 : {\"class\": \"content-meta__headline medium-headline\"},\n",
    "#         8 : {\"class\" : \"content-meta__headline__wrapper sc-1n74gvm-0 eYcrcn\"}\n",
    "        \n",
    "    }\n",
    "}\n",
    "\n",
    "#item branded-item branded-item--earther rlhps0-0 kQEtSU\n",
    "# page = (soup.find(class_=\"sc-1b65c8p-5 jOgcgy\"))\n",
    "# data = (soup.findAll(class_=\"item branded-item branded-item--gizmodo rlhps0-0 kQEtSU\"))\n",
    "# print(data)\n",
    "\n",
    "for i in range (1,7,2):\n",
    "    page = (soup.find(class_=selectors[\"https://gizmodo.com/\"][i][\"class\"]))\n",
    "    #print(page)\n",
    "    data = (soup.findAll(class_=selectors[\"https://gizmodo.com/\"][i+1][\"class\"]))\n",
    "\n",
    "titles = []\n",
    "        \n",
    "for page in data:\n",
    "#     title = page.find(\"a\",{\"class\":\"sc-1vk1s7l-1 dPODWC\"})\n",
    "#     title = page.find('h3')\n",
    "    title = page.find(\"a\", href=True).text\n",
    "    print(title)\n",
    "#     other = page.find(\"title\")\n",
    "#     print(other)\n",
    "#     title = page.find(\"a\", href=True).text\n",
    "    titles.append(title)\n",
    "#     title = page_f.find(\"a\", href=True).text\n",
    "#     titles.append(title)\n",
    "    \n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Title':titles}) \n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"relevant\"] = df.astype(str).sum(axis=1).str.contains(\"Libra\" or \"VC\")\n",
    "\n",
    "#feature to add for later: user enters specific keywords \n",
    "keywords = 'Facebook Google Microsoft Apple startup Linkedin'\n",
    "\n",
    "#case = False for case insensitivity \n",
    "df[\"relevant\"] = df.astype(str).sum(axis=1).str.contains(\"|\".join(keywords.split(\" \")),case = False)\n",
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.relevant != False] #dropping all false rows \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
